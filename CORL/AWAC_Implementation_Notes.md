# Notes

## How much data?

1. Offline datast $D^{(G)}$

   1. Generated by some "guide policy" (in our case we want a stabilizing policy)
   2. `config.offline_data.num_transitions` = `config.offline_data.num_rollouts * config.offline_data.rollout_length`
2. Offline Training

   1. Only uses data in $D^{(G)}$, but how much?
   2. The total amount of data passed into the training process is: `offline_training_data = config.offline_train.batch_size * config.offline_train.num_epochs`
      * Note: This can easily be greater than $|D^{(G)}|$ i.e. `config.offline_data.num_transitions`, meaning data is reused
3. Online Fine-tuning

   1. Each online training epoch $l$ generates `config.online_train.rollout_length` new transitions in sequence, and then updates the policy using a randomly sampled batch from $D^{(l)}=\tau^{(l)}\cup D^{(l-1)}$
   2. The amount of training data used in the online training process is: `online_training_data = config.online_train.batch_Size * config.online_train.num_epochs`
      and due to the random sampling process, a lot of this data will most likely be from the offline dataset if the $D_{on}^{l}< D_{off}$

## Stages of Training

1. Offline Data Collection: Guide Policy $\pi_G$ interacts with environment to generate dataset $D_G$
2. Offline Training of agent from Offline Data: Agent is trained on $D_G$ to successively generate policies $(\pi_\theta^{(-M)}, \pi_\theta^{(-M+1)}, ...., \pi^{(-1)})$.

- *Question: How do we evaluate the performance of policy $\pi^{(-M+l)}$ during the offline-training phase in practice? If we have a simulator, we can evaluate it. Otherwise how do we select the best policy when the final policy is not guaranteed to be the best*
- For now, we take the final policy $\pi_\theta^{(-1)}$ as the best policy and use it for online training

3. Online Training of agent:
   Let $\pi^{(0)}=\text{best}(\bar \pi^{\text{off}})$ or $\pi^{(0)}=\pi^{(-1)}$, and $D^{(0)}=D^G. Repeat:
   i.  Generate rollout $\tau^{(l)}$ using $\pi^{(l)}$
   ii. Set $D^{(i)}=\tau^{(l)}\cup D^{(l-1)}$
   iii. Train $\pi^{(l+1)}$ using $D^{(l+1)}$

### Offline Data Collection

Inputs

1. Guide Policy $\pi^G$
2. Number of rollouts to generate `num_rollouts`
3. Length of each rollout `rollout_length`

### Offline Training

Inputs:

1. Replay Buffer with offline data `replay_buffer`
2. Number of epochs `config.offline_train.num_epochs`
3. Batch size per epoch `config.offline_train.batch_size`
4. Number of epochs per evaluation `config.offline_train.eval_freq`

## Terminology

Offline Data = Data collected from the environment before training begins

## Quick Notes

Will assess the current performance of Offline + Fine-tuning

AWAC stores all of the online and offline data together, each being sampled with the sample probability. During Online Training, initially most of the samples will be from the Offline Training Data

# Tasking

1. Run on different environment
2. Run using different policy (shortest queue, randomized, etc)
3.
