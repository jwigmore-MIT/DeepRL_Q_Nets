---
config:
root_dir: ~
notes: "Testing with JRQ safe policy rollout calibration phase"
seed: 5031997
device: "cpu"
deterministic_torch: true
tasks:
  - "train"
  - "eval"
  - "test"
save_models: true

load:
  zip_path: saved_models/PPO_Env1a_TRAIN_20230621-123803/best_model

buffer:
  size: 6.0e+7
normalizers:
  obs:
    type: Fixed
    norm_factor: 20
    eps: 1.0e-6
  target:
    update_rate: ~
    eps: 1.0e-6

agent:
  policy_name: "SafeLTAPPO-JRQDiscreteActor"
  policy: "SafeLTAPPO-JRQDiscreteActor"
  safe_agent: true
  lta_agent: true
  actor:
    type: "JRQDiscreteActor"
    learning_rate: 3.0e-4
    std_learning_rate: 3.0e-03
    kwargs:
      hidden_dim: 64
      activation: "tanh"
      hidden_layers: 2
  safety:
    safe_policy: "JRQ"
    trigger_state: 20
    mod: ~
    mod_args:
      omega: 0.5
    args:
      M: "R"
  critic:
    learning_rate: 3.0e-04
    kwargs:
        hidden_dim: 64
        activation: "tanh"
        hidden_layers: 2
  n_epochs: 10
  kwargs:
    critic_first: true
    recompute_adv: false
    update_epochs: 10 # number of updates to perform per batch
    minibatches: 4 # number of minibatches to break the data up into
    alpha: 0.1
    nu: 1.0
    gamma: 1.0
    gae_lambda: 0.0
    clip_coef: 0.2
    kl_coef: 0.0
    ent_coef: 0.0
    kl_target: 1
    grad_clip: 10
    value_clip: 1
    vf_coef: 0.5
    imit_coef: 1.0
    pg_coef: 1.0
    int_coef: 0.0
    norm_adv: false
    pretrain_minibatches: 1
    pretrain_epochs: 1000
    omega_norm: true



env:
    env_json_path: "/JSON/Environment/ServerAssignment/N2S3.json"
    wrappers:
      normalize_obs: false
      normalize_reward: false
      record_stats: true
      scale_reward: ~

wandb:
    project: "N2S3"
    group: ~


train:
  num_episodes: 10000
  batch_size: 512
  reset: false
  pretrain_steps: 1000


eval:
  n_eval_episodes: 10
  deterministic: true





