---
config:
root_dir: ~
seed: 5031997
device: "cpu"
deterministic_torch: true
tasks:
  - "train"
  - "eval"
  - "test"
save_models: true

load:
  zip_path: saved_models/PPO_Env1a_TRAIN_20230621-123803/best_model

buffer:
  size: 1.0e+7
normalizers:
  obs:
    eps: 1.0e-6
  target:
    update_rate: 0.1
    eps: 1.0e-6

agent:
  policy_name: PPO-Beta
  policy: "PPO-Beta"
  safe_agent: false
  actor:
    type: "Beta"
    kwargs:
      hidden_dim: 64


  critic:
    hidden_dim: 64
  n_epochs: 10
  kwargs:
    update_epochs: 10 # number of updates to perform per batch
    minibatches: 4 # number of minibatches to break the data up into
    gamma: 0.99
    gae_lambda: 0.95
    clip_coef: 0.2
    kl_coef: 0.0
    ent_coef: 0.0
    kl_target: 1
    grad_clip: 0.5
    value_clip: 1
    vf_coef: 0.5


env:
    env_json_path: "/JSON/Environment/Env2a.json"
    wrappers:
      normalize_obs: false
      normalize_reward: false
      time_limit: 128
      record_stats: true

wandb:
    project: "Debug"
    group: "PPO-Gaussian"



monitor:
    filename: "PPO"
    info_keywords: ["backlog",]

train:
  num_episodes: 1000
  reset_timesteps: 256
  learning_rate: 3.0e-03
  batch_size: 256

eval:
  n_eval_episodes: 10
  deterministic: true





